{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424e6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377d523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'aircraft carrier', b'airplane', b'alarm clock', b'ambulance', b'angel', b'animal migration', b'ant', b'anvil', b'apple', b'arm']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('npy_files/candle.npy', <http.client.HTTPMessage at 0x7ff9e7911c90>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
    "print(categories[:10])\n",
    "category = 'candle'\n",
    "\n",
    "if not os.path.isdir('npy_files'):\n",
    "    os.mkdir('npy_files')\n",
    "    \n",
    "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
    "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4687bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141545 images to train on\n",
      "0 255\n"
     ]
    }
   ],
   "source": [
    "train_images = np.load(f'npy_files/{category}.npy')\n",
    "print(f'{len(train_images)} images to train on')\n",
    "print(np.min(train_images), np.max(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f37dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 19:13:56.605102: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-01-24 19:13:56.605130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: abacus\n",
      "2023-01-24 19:13:56.605136: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: abacus\n",
      "2023-01-24 19:13:56.605203: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 520.56.6\n",
      "2023-01-24 19:13:56.605222: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.56.6\n",
      "2023-01-24 19:13:56.605227: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 520.56.6\n",
      "2023-01-24 19:13:56.605493: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "\n",
    "def preprocess(data):\n",
    "    # DATA TYPE\n",
    "    data = data.map(lambda img: (tf.cast(img, tf.float32)))\n",
    "    # NORMALIZE\n",
    "    data = data.map(lambda img: (img/255.))\n",
    "    # DUPING AND NOISE\n",
    "    data = data.map(lambda img: (tf.reshape(img, (28, 28, 1))))\n",
    "    # DATAFLOW PREP\n",
    "    data = data.cache()\n",
    "    data = data.shuffle(1000, seed=42)\n",
    "    data = data.batch(32)\n",
    "    data = data.prefetch(3)\n",
    "    return data\n",
    "\n",
    "dataset = dataset.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fc89a1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 112, 1])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = next(iter(dataset))\n",
    "\n",
    "x = tf.concat([x[i, ...] for i in range(4)], axis=1)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(x, cmap='binary')\n",
    "plt.show()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "df2267e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]], shape=(3, 1), dtype=float32) tf.Tensor(0.69314694, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense  = tf.keras.layers.Dense(14 * 14 * 128)\n",
    "        self.shape  = tf.keras.layers.Reshape((14, 14, 128))\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same', activation='leaky_relu')\n",
    "        self.conv_2 = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same', activation='leaky_relu')\n",
    "        self.norm_0 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv_3 = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=5, padding='same', activation='leaky_relu')\n",
    "        self.conv_4 = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=5, padding='same', activation='leaky_relu')\n",
    "        self.norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv_5 = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=5, padding='same', activation='leaky_relu')\n",
    "        self.conv_6 = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=5, padding='same', activation='leaky_relu')\n",
    "        self.norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv_7 = tf.keras.layers.SeparableConv2D(filters=1, kernel_size=5, padding='same')\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=4e-4, clipvalue=1.0, decay=1e-8)\n",
    "        \n",
    "        \n",
    "    def call(self, batch_size, training=True):\n",
    "        X = tf.random.normal((batch_size, 256))\n",
    "        X = self.dense(X)\n",
    "        X = self.shape(X)\n",
    "        # 1st BLOCK\n",
    "        X = self.conv_1(X, training=training)\n",
    "        Z = self.conv_2(X, training=training)\n",
    "        X = self.norm_0(Z, training=training)\n",
    "        # 2nd BLOCK\n",
    "        X = self.conv_3(X, training=training)\n",
    "        Z = self.conv_4(X, training=training)\n",
    "        #Z = Z + X\n",
    "        X = self.norm_1(Z, training=training)\n",
    "        # 3rd BLOCK\n",
    "        #X = self.conv_5(X, training=training)\n",
    "        #Z = self.conv_6(X, training=training)\n",
    "        #Z = Z + X\n",
    "        #X = self.norm_2(Z, training=training)\n",
    "        X = self.conv_7(Z, training=training)\n",
    "        return X\n",
    "        \n",
    "\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_0 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='leaky_relu')\n",
    "        # (28, 28, 64)\n",
    "        self.conv_1 = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=4, strides=2, padding='same', activation='leaky_relu')\n",
    "        # (14, 14, 64)\n",
    "        self.conv_2 = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=4, strides=2, padding='same', activation='leaky_relu')\n",
    "        # (7, 7, 64)\n",
    "        self.conv_3 = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=4, strides=2, padding='same', activation='leaky_relu')\n",
    "        # (3, 3, 64)\n",
    "        self.conv_4 = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='leaky_relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.4)\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=8e-4, clipvalue=1.0, decay=1e-8)\n",
    "    \n",
    "    \n",
    "    def call(self, X, training=True):\n",
    "        # Constructing Filterspace\n",
    "        X = self.conv_0(X, training=training)\n",
    "        X = self.conv_1(X, training=training)\n",
    "        X = self.conv_2(X, training=training)\n",
    "        X = self.conv_3(X, training=training)\n",
    "        X = self.conv_4(X, training=training)\n",
    "        # FINALLY\n",
    "        X = self.flatten(X)\n",
    "        X = self.dropout(X, training=training)\n",
    "        P = self.dense(X)\n",
    "        return P\n",
    "\n",
    "\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "n = 3\n",
    "P = D(G(n))\n",
    "L = tf.keras.losses.BinaryCrossentropy()(tf.zeros(n), P)\n",
    "print(P, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "715f0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "\n",
    "\n",
    "\n",
    "class GAN(tf.keras.Model): # aka Adversarial Minmax\n",
    "    NAME = 'GENERATIVE ADVERSARIAL NETWORK'\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.G = Generator()\n",
    "        self.D = Discriminator()\n",
    "        self.is_setup = False\n",
    "        self.loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.prev_epochs = 0\n",
    "    \n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "    \n",
    "    \n",
    "    def setup(self):\n",
    "        if self.is_setup:\n",
    "            return\n",
    "        # DEFINE PATHSf\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        train_log_path = f\"logs/{self.NAME}{current_time}\"\n",
    "        # CONSTRUCT WRITERS\n",
    "        self.writer = tf.summary.create_file_writer(train_log_path)\n",
    "        self.is_setup = True\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X_true, training=True):\n",
    "        batch_size = X_true.shape[0]\n",
    "        X_false = self.G(batch_size, training=training)\n",
    "        X = tf.concat([X_true, X_false], axis=0)\n",
    "        P = self.D(X, training=training)\n",
    "        return P\n",
    "    \n",
    "    \n",
    "    #@tf.function\n",
    "    def step(self, X_true, training=True):\n",
    "        batch_size = X_true.shape[0]\n",
    "        T = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "        if training:\n",
    "            with tf.GradientTape() as tape:\n",
    "                P = self(X_true, training=training)\n",
    "                L_D = self.loss(T, P) + tf.random.uniform(P.shape, maxval=0.05)\n",
    "            nabla_D = tape.gradient(L_D, self.D.trainable_weights)\n",
    "            self.D.optimizer.apply_gradients(zip(nabla_D, self.D.trainable_weights))\n",
    "            with tf.GradientTape() as tape:\n",
    "                P = self(X_true, training=training)\n",
    "                L_G = -self.loss(T, P)\n",
    "            nabla_G = tape.gradient(L_G, self.G.trainable_weights)\n",
    "            self.G.optimizer.apply_gradients(zip(nabla_G, self.G.trainable_weights))\n",
    "        else:\n",
    "            P = self(X_true)\n",
    "            L_D = self.loss(T, P)\n",
    "        self.loss_metric.update_state(L_D)\n",
    "        return {metric.name: float(metric.result()) for metric in self.metrics}\n",
    "\n",
    "\n",
    "    def train(self, dataset, epochs):\n",
    "        training_metrics = {metric.name: [] for metric in self.metrics}\n",
    "        testing_metrics = {metric.name: [] for metric in self.metrics}\n",
    "        self.setup()\n",
    "        with tqdm(epochs) as bar:\n",
    "            bar.set_description('TRAINING')\n",
    "            for epoch in range(epochs):\n",
    "                # TRAINING\n",
    "                for X_true in dataset:\n",
    "                    metrics = self.step(X_true, training=True)\n",
    "                    for name, value in metrics.items():\n",
    "                        training_metrics[name].append(value)\n",
    "                # WRITING METRICS\n",
    "                with self.writer.as_default():\n",
    "                    for metric in self.metrics:\n",
    "                        tf.summary.scalar(metric.name, metric.result(), step=self.prev_epochs)\n",
    "                    tf.summary.image('Image', self.G(1)[0:])\n",
    "                self.reset_metrics()\n",
    "                bar.update(1)\n",
    "                self.prev_epochs += 1\n",
    "            metrics = {metric.name: [training_metrics[metric.name], testing_metrics[metric.name]] for metric in self.metrics}\n",
    "            return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "51da7f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['generator_106/batch_normalization_595/gamma:0', 'generator_106/batch_normalization_595/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: : 0it [03:25, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [213], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#model.D.load_weights('discriminator.pd')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model.G.load_weights('generator.pd')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [212], line 75\u001b[0m, in \u001b[0;36mGAN.train\u001b[0;34m(self, dataset, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# TRAINING\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_true \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 75\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     77\u001b[0m             training_metrics[name]\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "Cell \u001b[0;32mIn [212], line 50\u001b[0m, in \u001b[0;36mGAN.step\u001b[0;34m(self, X_true, training)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 50\u001b[0m         P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         L_D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(T, P) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(P\u001b[38;5;241m.\u001b[39mshape, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m     52\u001b[0m     nabla_D \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(L_D, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD\u001b[38;5;241m.\u001b[39mtrainable_weights)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1099\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1861\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n\u001b[0;32m-> 1861\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mforward_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_tangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1864\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_override_gradient_function(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1865\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function(),\n\u001b[1;32m   1866\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatefulPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function()}):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GAN()\n",
    "#model.D.load_weights('discriminator.pd')\n",
    "#model.G.load_weights('generator.pd')\n",
    "with tf.device('GPU'):\n",
    "    metrics = model.train(dataset, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff5f5d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "796a0e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = model.G(3)\n",
    "x = tf.concat([x[i, ...] for i in range(3)], axis=1)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(x, cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6b0a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.D.save_weights('discriminator.pd')\n",
    "model.G.save_weights('generator.pd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
