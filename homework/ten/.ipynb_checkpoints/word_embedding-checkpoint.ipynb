{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f4533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from math import sqrt\n",
    "from random import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, file, vocab_size, embedding_size, n_negative=7):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_negative = n_negative\n",
    "        # READING CORPUS\n",
    "        with open(file, 'r') as file:\n",
    "            self.text = file.read()\n",
    "        # CLEANING TEXT\n",
    "        text = self.text.lower()\n",
    "        symbols = set(text)\n",
    "        exclude = '[:0123456789*;,()-\\']'\n",
    "        text = re.sub('?'.join(exclude), '', text)\n",
    "        text = re.sub('\\t?\\r?\\n', ' ', text)\n",
    "        words = text.replace('.', '').split(' ')\n",
    "        # SUBSAMPLING\n",
    "        frequencies = {word: words.count(word)for word in set(words)}\n",
    "        n_words = sum(frequencies.values())\n",
    "        P = lambda x, s=0.001: (sqrt(frequencies[x]/(n_words * s)) + 1) * s * n_words/frequencies[x]\n",
    "        sentences = [[word for word in sentence.split(' ') if P(word) > random() and word] for sentence in text.split('.')]\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
    "        self.tokenizer.fit_on_texts([text.replace('.', ' ')])\n",
    "        # CREATING DATASET\n",
    "        dataset = []\n",
    "        for sentence in sentences:\n",
    "            sequence = self.tokenizer.texts_to_sequences(sentence)\n",
    "            pairs, labels = tf.keras.preprocessing.sequence.skipgrams(sequence, \n",
    "                                                              vocabulary_size=10000, \n",
    "                                                              window_size=1, \n",
    "                                                              negative_samples=0.0)\n",
    "    \n",
    "            for target, context in pairs:\n",
    "                if len(context) != 1:\n",
    "                    context = context[0]\n",
    "                context = tf.reshape(tf.constant(context, dtype=tf.int64), (1, 1))\n",
    "                context_negative, true_count, sampled_count = tf.random.log_uniform_candidate_sampler(true_classes=context,\n",
    "                                                                num_true=1,\n",
    "                                                                num_sampled=self.n_negative,\n",
    "                                                                unique=True,\n",
    "                                                                range_max=10000)\n",
    "                context = tf.concat([context[0], context_negative], axis=0)\n",
    "                labels = tf.constant([1] + [0] * self.n_negative, dtype=tf.int64)\n",
    "                dataset.append(tf.concat([tf.constant([target[0]], dtype=tf.int64), context, labels], axis=0))\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "        def preprocess(data):\n",
    "            data = data.map(lambda x: tf.cast(x, tf.uint32))\n",
    "            data = data.map(lambda x: (x[0], x[1:2 + n_negative], tf.cast(x[2 + n_negative:], tf.float32)))\n",
    "            data = data.shuffle(42)\n",
    "            data = data.cache()\n",
    "            data = data.batch(32)\n",
    "            data = data.prefetch(20)\n",
    "            return data\n",
    "        self.dataset = self.dataset.apply(preprocess)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "        self.build()\n",
    "    \n",
    "    \n",
    "    def build(self, *args):\n",
    "        self.W_t = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                             self.embedding_size,\n",
    "                                            input_length=1)\n",
    "        self.W_c = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                             self.embedding_size,\n",
    "                                            input_length=1)\n",
    "        self.is_built = True\n",
    "    \n",
    "    \n",
    "    def call(self, target, context):\n",
    "        embed_t = self.W_t(target)\n",
    "        embed_c = self.W_c(context)\n",
    "        return tf.einsum('be,bce->bc', embed_t, embed_c)\n",
    "    \n",
    "    \n",
    "    def loss(self, logits, labels):\n",
    "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    \n",
    "    \n",
    "    def train(self, epochs):\n",
    "        with tqdm(total=epochs) as bar:\n",
    "            bar.set_description('JAZZ')\n",
    "            for epoch in range(epochs):\n",
    "                for target, context, labels in self.dataset:\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        logits = self(target, context)\n",
    "                        loss   = self.loss(logits, labels)\n",
    "                    gradients = tape.gradient(loss, self.trainable_weights)\n",
    "                    self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "\n",
    "\n",
    "model = Embedding('bible.txt', 10000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc672654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAZZ:   0%|                                                                    | 0/10 [1:08:42<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42a384ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him', 'not', 'them', 'it', 'with', 'all', 'thou', 'thy', 'was', 'god', 'which', 'my', 'me', 'said', 'but', 'ye', 'their', 'have', 'will', 'thee', 'from', 'as', 'are', 'when', 'this', 'out', 'were', 'upon', 'by', 'man', 'you', 'israel', 'up', 'son', 'there', 'hath', 'king', 'then', 'people', 'came', 'had', 'house', 'into', 'on', 'her', 'come', 'one', 'we', 'children', 'before', 'your', 'also', 'day', 'land', 'an', 'so', 'against', 'men', 'shalt', 'if', 'at', 'let', 'go', 'hand', 'saying', 'us', 'made', 'went', 'no', 'even', 'do', 'now', 'behold', 'saith', 'therefore', 'every', 'these', 'because', 'after', 'our', 'things', 'down', 'or']\n"
     ]
    }
   ],
   "source": [
    "with open('bible.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()\n",
    "symbols = set(text)\n",
    "exclude = '[:0123456789*;,()-\\']'\n",
    "text = re.sub('?'.join(exclude), '', text)\n",
    "text = re.sub('\\t?\\r?\\n', ' ', text)\n",
    "words = text.replace('.', '').split(' ')\n",
    "frequencies = {word: words.count(word)for word in set(words)}\n",
    "top_100 = [key for key, val in sorted(frequencies.items(), key=lambda x: -x[1]) if key][:100]\n",
    "print(top_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c4f881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = model.W_t.weights[0].numpy()\n",
    "\n",
    "embed = lambda w: model.W_t(model.tokenizer.texts_to_sequences([w])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1af1c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'i'\tneighbors:\ti we they ye indeed\n",
      "'shall'\tneighbors:\tshall may should will shalt\n",
      "'say'\tneighbors:\tsay speak said saith saying\n",
      "'the'\tneighbors:\tthe of reconciling selah whose\n",
      "'king'\tneighbors:\treign then acts days governor\n",
      "'is'\tneighbors:\twas art availeth worketh defileth\n",
      "'naked'\tneighbors:\tnurse kittim fornications girding tail\n"
     ]
    }
   ],
   "source": [
    "def cosine_distance(A, B):\n",
    "    tf.math.reduce_euclidean_norm(A)\n",
    "    tf.math.reduce_euclidean_norm(B, axis=1)\n",
    "    return tf.math.acos(tf.reduce_sum(A * B, axis=1)/\\\n",
    "    (tf.math.reduce_euclidean_norm(A) * tf.math.reduce_euclidean_norm(B, axis=1)))\n",
    "\n",
    "fun_words = ['i', 'shall', 'say', 'the', 'king', 'is', 'naked']\n",
    "k = 5\n",
    "\n",
    "for word in fun_words:\n",
    "    vec = embed(word)\n",
    "    idx = tf.argsort(cosine_distance(vec, M))[:k,...].numpy()\n",
    "    print(f\"'{word}'\\tneighbors:\\t{model.tokenizer.sequences_to_texts([idx])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36145e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
