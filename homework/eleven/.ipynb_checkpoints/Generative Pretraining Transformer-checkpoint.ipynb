{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f160e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  16  134  965  535   30  194   11   14 1587], shape=(9,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: capital.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4096\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: capital.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (4274 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 12147 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 30 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=4622453\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=81\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 11858 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 58191 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 11858\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 39397\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 39397 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=21470 obj=10.1597 num_tokens=82346 num_tokens/piece=3.8354\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=18058 obj=7.74947 num_tokens=82620 num_tokens/piece=4.57526\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13534 obj=7.73745 num_tokens=87722 num_tokens/piece=6.4816\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13525 obj=7.72217 num_tokens=87750 num_tokens/piece=6.48799\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=10143 obj=7.80088 num_tokens=95910 num_tokens/piece=9.45578\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=10143 obj=7.78361 num_tokens=95914 num_tokens/piece=9.45618\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=7607 obj=7.90441 num_tokens=105686 num_tokens/piece=13.8933\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=7607 obj=7.88253 num_tokens=105686 num_tokens/piece=13.8933\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5705 obj=8.03965 num_tokens=116254 num_tokens/piece=20.3776\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5705 obj=8.01254 num_tokens=116251 num_tokens/piece=20.377\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4505 obj=8.16814 num_tokens=124801 num_tokens/piece=27.7028\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4505 obj=8.14119 num_tokens=124810 num_tokens/piece=27.7048\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: tokenizer_model.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tft\n",
    "import numpy as np\n",
    "import sentencepiece as sp\n",
    "\n",
    "\n",
    "vocab_size = 4096\n",
    "corpus_file = 'capital.txt'\n",
    "\n",
    "sp.SentencePieceTrainer.train(input=corpus_file, model_prefix='tokenizer_model', \n",
    "                              model_type=\"unigram\", vocab_size=vocab_size)\n",
    "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
    "tokenizer = tft.SentencepieceTokenizer(model=trained_tokenizer_model, out_type=tf.int32, \n",
    "                                          nbest_size=-1, alpha=1, reverse=False, add_bos=False, \n",
    "                                          add_eos=False, return_nbest=False, name=None)\n",
    "tokens = tokenizer.tokenize(\"thou shall not do a capitalism\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f73ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 33#65\n",
    "\n",
    "\n",
    "with open(corpus_file, 'r') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus_tokens = tokenizer.tokenize(corpus)\n",
    "windows = tft.sliding_window(corpus_tokens, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44b9d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 32), dtype=int32, numpy=\n",
       " array([[1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,   74, 2804,\n",
       "            9, 1116,   82,  911,  149,   74, 2804, 1709,  149,   74,   24,\n",
       "         3008, 1883,  167,    8,    5,   11, 3416,   74, 3993,   13],\n",
       "        [  94,   82, 3397, 1419,    5, 1653, 1509, 2364,  600,   39,   80,\n",
       "         1378,  879,   74,   24, 1028,   57, 1006,    5, 1631,    5,  644,\n",
       "         1406,  149,   74, 2804,    9, 1116,   82,  911,  149,   74],\n",
       "        [ 911,  149,   74, 2804, 1709,  149,   74,   24, 3008, 1883,  167,\n",
       "            8,    5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,\n",
       "          481,  443, 1332,  299,  504,    5, 1918,    9,    3,  107],\n",
       "        [1509, 2364,  600,   39,   80, 1378,  879,   74,   24, 1028,   57,\n",
       "         1006,    5, 1631,    5,  644, 1406,  149,   74, 2804,    9, 1116,\n",
       "           82,  911,  149,   74, 2804, 1709,  149,   74,   24, 3008],\n",
       "        [ 644, 1406,  149,   74, 2804,    9, 1116,   82,  911,  149,   74,\n",
       "         2804, 1709,  149,   74,   24, 3008, 1883,  167,    8,    5,   11,\n",
       "         3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,  443],\n",
       "        [   5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,\n",
       "          443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5,\n",
       "         1918,   81,   24,  494,    5,  223, 3492,    7,   21,    3],\n",
       "        [  82, 3397, 1419,    5, 1653, 1509, 2364,  600,   39,   80, 1378,\n",
       "          879,   74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,\n",
       "          149,   74, 2804,    9, 1116,   82,  911,  149,   74, 2804],\n",
       "        [  74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,\n",
       "           74, 2804,    9, 1116,   82,  911,  149,   74, 2804, 1709,  149,\n",
       "           74,   24, 3008, 1883,  167,    8,    5,   11, 3416,   74],\n",
       "        [  24, 3008, 1883,  167,    8,    5,   11, 3416,   74, 3993,   13,\n",
       "         2570,    9, 1918,   48,  481,  443, 1332,  299,  504,    5, 1918,\n",
       "            9,    3,  107, 4007,    5, 1918,   81,   24,  494,    5],\n",
       "        [1378,  879,   74,   24, 1028,   57, 1006,    5, 1631,    5,  644,\n",
       "         1406,  149,   74, 2804,    9, 1116,   82,  911,  149,   74, 2804,\n",
       "         1709,  149,   74,   24, 3008, 1883,  167,    8,    5,   11],\n",
       "        [ 167,    8,    5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,\n",
       "           48,  481,  443, 1332,  299,  504,    5, 1918,    9,    3,  107,\n",
       "         4007,    5, 1918,   81,   24,  494,    5,  223, 3492,    7],\n",
       "        [ 149,   74, 2804, 1709,  149,   74,   24, 3008, 1883,  167,    8,\n",
       "            5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,\n",
       "          443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007],\n",
       "        [ 443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5,\n",
       "         1918,   81,   24,  494,    5,  223, 3492,    7,   21,    3,   58,\n",
       "          272,    5,   25, 2487,    8,    4, 2020,  118,   15,   60],\n",
       "        [3397, 1419,    5, 1653, 1509, 2364,  600,   39,   80, 1378,  879,\n",
       "           74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,\n",
       "           74, 2804,    9, 1116,   82,  911,  149,   74, 2804, 1709],\n",
       "        [   8,    5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,\n",
       "          481,  443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007,\n",
       "            5, 1918,   81,   24,  494,    5,  223, 3492,    7,   21],\n",
       "        [   9, 1918,   48,  481,  443, 1332,  299,  504,    5, 1918,    9,\n",
       "            3,  107, 4007,    5, 1918,   81,   24,  494,    5,  223, 3492,\n",
       "            7,   21,    3,   58,  272,    5,   25, 2487,    8,    4]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(16, 32), dtype=int32, numpy=\n",
       " array([[  57, 1006,    5, 1631,    5,  644, 1406,  149,   74, 2804,    9,\n",
       "         1116,   82,  911,  149,   74, 2804, 1709,  149,   74,   24, 3008,\n",
       "         1883,  167,    8,    5,   11, 3416,   74, 3993,   13, 2570],\n",
       "        [  82, 3397, 1419,    5, 1653, 1509, 2364,  600,   39,   80, 1378,\n",
       "          879,   74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,\n",
       "          149,   74, 2804,    9, 1116,   82,  911,  149,   74, 2804],\n",
       "        [ 149,   74, 2804, 1709,  149,   74,   24, 3008, 1883,  167,    8,\n",
       "            5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,\n",
       "          443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007],\n",
       "        [2364,  600,   39,   80, 1378,  879,   74,   24, 1028,   57, 1006,\n",
       "            5, 1631,    5,  644, 1406,  149,   74, 2804,    9, 1116,   82,\n",
       "          911,  149,   74, 2804, 1709,  149,   74,   24, 3008, 1883],\n",
       "        [1406,  149,   74, 2804,    9, 1116,   82,  911,  149,   74, 2804,\n",
       "         1709,  149,   74,   24, 3008, 1883,  167,    8,    5,   11, 3416,\n",
       "           74, 3993,   13, 2570,    9, 1918,   48,  481,  443, 1332],\n",
       "        [  11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,  443,\n",
       "         1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5, 1918,\n",
       "           81,   24,  494,    5,  223, 3492,    7,   21,    3,   58],\n",
       "        [3397, 1419,    5, 1653, 1509, 2364,  600,   39,   80, 1378,  879,\n",
       "           74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,\n",
       "           74, 2804,    9, 1116,   82,  911,  149,   74, 2804, 1709],\n",
       "        [  24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,   74,\n",
       "         2804,    9, 1116,   82,  911,  149,   74, 2804, 1709,  149,   74,\n",
       "           24, 3008, 1883,  167,    8,    5,   11, 3416,   74, 3993],\n",
       "        [3008, 1883,  167,    8,    5,   11, 3416,   74, 3993,   13, 2570,\n",
       "            9, 1918,   48,  481,  443, 1332,  299,  504,    5, 1918,    9,\n",
       "            3,  107, 4007,    5, 1918,   81,   24,  494,    5,  223],\n",
       "        [ 879,   74,   24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,\n",
       "          149,   74, 2804,    9, 1116,   82,  911,  149,   74, 2804, 1709,\n",
       "          149,   74,   24, 3008, 1883,  167,    8,    5,   11, 3416],\n",
       "        [   8,    5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,\n",
       "          481,  443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007,\n",
       "            5, 1918,   81,   24,  494,    5,  223, 3492,    7,   21],\n",
       "        [  74, 2804, 1709,  149,   74,   24, 3008, 1883,  167,    8,    5,\n",
       "           11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,  443,\n",
       "         1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5],\n",
       "        [1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5, 1918,\n",
       "           81,   24,  494,    5,  223, 3492,    7,   21,    3,   58,  272,\n",
       "            5,   25, 2487,    8,    4, 2020,  118,   15,   60,  346],\n",
       "        [1419,    5, 1653, 1509, 2364,  600,   39,   80, 1378,  879,   74,\n",
       "           24, 1028,   57, 1006,    5, 1631,    5,  644, 1406,  149,   74,\n",
       "         2804,    9, 1116,   82,  911,  149,   74, 2804, 1709,  149],\n",
       "        [   5,   11, 3416,   74, 3993,   13, 2570,    9, 1918,   48,  481,\n",
       "          443, 1332,  299,  504,    5, 1918,    9,    3,  107, 4007,    5,\n",
       "         1918,   81,   24,  494,    5,  223, 3492,    7,   21,    3],\n",
       "        [1918,   48,  481,  443, 1332,  299,  504,    5, 1918,    9,    3,\n",
       "          107, 4007,    5, 1918,   81,   24,  494,    5,  223, 3492,    7,\n",
       "           21,    3,   58,  272,    5,   25, 2487,    8,    4, 2020]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(windows)\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = data.map(lambda x: (x[:-1], x[1:]))\n",
    "    data = data.shuffle(42)\n",
    "    data = data.batch(16)\n",
    "    #data = data.cache()\n",
    "    data = data.prefetch(20)\n",
    "    return data\n",
    "\n",
    "dataset = dataset.apply(preprocessing)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1063e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.is_setup = False\n",
    "        self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"acc\")\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.prev_epochs = 0\n",
    "    \n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "    \n",
    "    \n",
    "    def setup(self):\n",
    "        if self.is_setup:\n",
    "            return\n",
    "        # DEFINE PATHS\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        path = f\"logs/{current_time}\"\n",
    "        # CONSTRUCT WRITERS\n",
    "        self.writer_train = tf.summary.create_file_writer(path)\n",
    "        self.is_setup = True\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, X, T, training=True):\n",
    "        if training:\n",
    "            with tf.GradientTape() as tape:\n",
    "                Y = self(X, training=training)\n",
    "                L = self.loss(T, Y)\n",
    "            gradient = tape.gradient(L, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(gradient, self.trainable_weights))\n",
    "        else:\n",
    "            Y = self(X, training)\n",
    "            L = self.loss(T, Y)\n",
    "        self.loss_metric.update_state(L)\n",
    "        #self.accuracy_metric.update_state(T, Y)\n",
    "        return {metric.name: float(metric.result()) for metric in self.metrics}\n",
    "    \n",
    "    \n",
    "    def train(self, dataset, epochs):\n",
    "        training_metrics = {metric.name: [] for metric in self.metrics}\n",
    "        self.setup()\n",
    "        with self.writer_train.as_default():\n",
    "            tf.summary.text('text', self.generate_text('The', output_length=16, top_k=64), step=0)\n",
    "        with tqdm(epochs) as bar:\n",
    "            for epoch in range(epochs):\n",
    "                # TRAINING\n",
    "                bar.set_description('TRAINING')\n",
    "                for X, T  in dataset:\n",
    "                    metrics = self.step(X, T, training=True)\n",
    "                    for name, value in metrics.items():\n",
    "                        training_metrics[name].append(value)\n",
    "                # WRITING METRICS\n",
    "                with self.writer_train.as_default():\n",
    "                    for metric in self.metrics:\n",
    "                        tf.summary.scalar(metric.name, metric.result(), step=self.prev_epochs)\n",
    "                    tf.summary.text('text', self.generate_text('The', output_length=16, top_k=64), step=self.prev_epochs)\n",
    "                self.reset_metrics()\n",
    "                bar.update(1)\n",
    "                self.prev_epochs += 1\n",
    "            metrics = {metric.name: training_metrics[metric.name] for metric in self.metrics}\n",
    "            return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e6cb4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"gpt_1\" \"                 f\"(type GPT).\n\nin user code:\n\n    File \"/tmp/ipykernel_140399/3207563206.py\", line 114, in call  *\n        X = block(X, training=training)\n    File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file3nupcjvh.py\", line 11, in tf__call\n        Z = ag__.converted_call(ag__.ld(self).multihead, (ag__.ld(Z),), dict(training=ag__.ld(training)), fscope)\n\n    TypeError: Exception encountered when calling layer \"block_8\" \"                 f\"(type Block).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_140399/3207563206.py\", line 67, in call  *\n            Z = self.multihead(Z,  training=training)\n        File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n            return fn(*args, **kwargs)\n    \n        TypeError: MultiHeadAttention.call() missing 1 required positional argument: 'value'\n    \n    \n    Call arguments received by layer \"block_8\" \"                 f\"(type Block):\n      • X=tf.Tensor(shape=(1, 1, 256), dtype=float32)\n      • training=False\n\n\nCall arguments received by layer \"gpt_1\" \"                 f\"(type GPT):\n  • X=tf.Tensor(shape=(1, 1), dtype=int32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT(tokenizer, vocab_size, n_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#model.load_weights('GPT_V1')\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 126\u001b[0m, in \u001b[0;36mGPT.generate_text\u001b[0;34m(self, prompt, output_length, top_k)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_length):\n\u001b[1;32m    125\u001b[0m     X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(tokens, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)[\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m--> 126\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m    127\u001b[0m     top_val, top_idx \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mtop_k(logits, k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m    128\u001b[0m     idx \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(top_val[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileyxr21xzb.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, X, training)\u001b[0m\n\u001b[1;32m     23\u001b[0m     X \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(block), (ag__\u001b[38;5;241m.\u001b[39mld(X),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     24\u001b[0m block \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm, (ag__\u001b[38;5;241m.\u001b[39mld(X),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39maffine, (ag__\u001b[38;5;241m.\u001b[39mld(X),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileyxr21xzb.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m X\n\u001b[1;32m     22\u001b[0m block \u001b[38;5;241m=\u001b[39m itr\n\u001b[0;32m---> 23\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3nupcjvh.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, X, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m Z \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnorm_one, (ag__\u001b[38;5;241m.\u001b[39mld(X),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m---> 11\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m Z \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop_two, (ag__\u001b[38;5;241m.\u001b[39mld(Z),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     13\u001b[0m Z \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(X) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(Z)\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"gpt_1\" \"                 f\"(type GPT).\n\nin user code:\n\n    File \"/tmp/ipykernel_140399/3207563206.py\", line 114, in call  *\n        X = block(X, training=training)\n    File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file3nupcjvh.py\", line 11, in tf__call\n        Z = ag__.converted_call(ag__.ld(self).multihead, (ag__.ld(Z),), dict(training=ag__.ld(training)), fscope)\n\n    TypeError: Exception encountered when calling layer \"block_8\" \"                 f\"(type Block).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_140399/3207563206.py\", line 67, in call  *\n            Z = self.multihead(Z,  training=training)\n        File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/mortimer/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n            return fn(*args, **kwargs)\n    \n        TypeError: MultiHeadAttention.call() missing 1 required positional argument: 'value'\n    \n    \n    Call arguments received by layer \"block_8\" \"                 f\"(type Block):\n      • X=tf.Tensor(shape=(1, 1, 256), dtype=float32)\n      • training=False\n\n\nCall arguments received by layer \"gpt_1\" \"                 f\"(type GPT):\n  • X=tf.Tensor(shape=(1, 1), dtype=int32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_units, drop_rate=0.1, mask=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_units = n_units\n",
    "        self.mask = mask\n",
    "        self.drop = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.W_Q  = tf.keras.layers.Dense(self.n_units, use_bias=False, \n",
    "                                          kernel_regularizer=tf.keras.regularizers.L2())\n",
    "        self.W_K  = tf.keras.layers.Dense(self.n_units, use_bias=False, \n",
    "                                          kernel_regularizer=tf.keras.regularizers.L2())\n",
    "        self.W_V  = tf.keras.layers.Dense(self.n_units, use_bias=False, \n",
    "                                          kernel_regularizer=tf.keras.regularizers.L2())\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X, C=None, training=True): # X: Values, C: Context\n",
    "        C = X if C is None else C\n",
    "        dk = X.shape[1]\n",
    "        Q = self.W_Q(C)\n",
    "        K = self.W_K(C)\n",
    "        V = self.W_V(X)\n",
    "        A = tf.einsum('bik, bjk -> bij', Q, K) / tf.math.sqrt(tf.cast(dk, tf.float32))\n",
    "        if self.mask:\n",
    "            mask = tf.constant([[[1] * (i + 1) + [0] * (dk - i - 1) for i in range(dk)]], dtype=tf.float32)\n",
    "            A = A * mask\n",
    "        A = tf.nn.softmax(A, axis=2)\n",
    "        A = self.drop(A, training=training)\n",
    "        return tf.einsum('bij, bjk -> bik', A, V)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_units, n_heads, drop_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_units = n_units\n",
    "        self.n_heads = n_heads\n",
    "        self.linear = tf.keras.layers.Dense(n_units)\n",
    "        self.heads = [Attention(n_units, drop_rate) for _ in range(n_heads)]\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X, C=None, training=True): # X: Values, C: Context\n",
    "        C = X if C is None else C\n",
    "        Y = tf.concat([head(X, C, training=training) for head in self.heads], axis=2)\n",
    "        Z = self.linear(Y)\n",
    "        return Z\n",
    "\n",
    "\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_units, n_heads, drop_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #self.multihead  = MultiHeadAttention(n_units, n_heads, drop_rate)\n",
    "        self.multihead  = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=n_units, dropout=drop_rate)\n",
    "        self.norm_one   = tf.keras.layers.BatchNormalization()\n",
    "        self.norm_two   = tf.keras.layers.BatchNormalization()\n",
    "        self.affine_one = tf.keras.layers.Dense(n_units * 4)\n",
    "        self.affine_two = tf.keras.layers.Dense(n_units)\n",
    "        self.drop_one   = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.drop_two   = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.drop_three = tf.keras.layers.Dropout(drop_rate)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X, training=False):\n",
    "        Z = self.norm_one(X,   training=training)\n",
    "        Z = self.multihead(Z, Z, Z, use_causal_mask=True, training=training)\n",
    "        #Z = self.multihead(Z,  training=training)\n",
    "        Z = self.drop_two(Z,   training=training)\n",
    "        Z = X + Z\n",
    "        X = self.norm_one(Z,   training=training)\n",
    "        Z = self.affine_one(X)\n",
    "        Z = tf.nn.gelu(Z)\n",
    "        Z = self.affine_two(Z)\n",
    "        Z = self.drop_three(Z, training=training)\n",
    "        return X + Z\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embed = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed  = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X):\n",
    "        word_embed = self.word_embed(X)\n",
    "        pos_embed  = self.pos_embed(tf.range(X.shape[1]))\n",
    "        return word_embed + pos_embed\n",
    "        \n",
    "        \n",
    "\n",
    "class GPT(Model):\n",
    "    def __init__(self, tokenizer, n_vocab, n_units, n_layers, n_heads, drop_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = Embedding(n_vocab, n_units)\n",
    "        self.drop = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.blocks = [Block(n_units, n_heads, drop_rate) for _ in range(n_layers)]\n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        self.affine = tf.keras.layers.Dense(n_vocab)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, X, training=False):\n",
    "        X = self.embedding(X)\n",
    "        X = self.drop(X, training=training)\n",
    "        for block in self.blocks:\n",
    "            X = block(X, training=training)\n",
    "        X = self.norm(X, training=training)\n",
    "        X = self.affine(X)\n",
    "        if not training:\n",
    "            X = tf.nn.softmax(X, axis=2)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def generate_text(self, prompt: str, output_length: int, top_k: int):\n",
    "        tokens = tokenizer.tokenize(prompt).numpy().tolist()\n",
    "        for _ in range(output_length):\n",
    "            X = tf.constant(tokens, dtype=tf.int32)[None,...]\n",
    "            logits = self(X)[0,-1,:]\n",
    "            top_val, top_idx = tf.math.top_k(logits, k=top_k)\n",
    "            idx = tf.random.categorical(top_val[None, ...], 1)[0, 0]\n",
    "            token = top_idx[idx].numpy()\n",
    "            tokens.append(token)\n",
    "        return self.tokenizer.detokenize(tokens)\n",
    "\n",
    "\n",
    "\n",
    "model = GPT(tokenizer, vocab_size, n_units=256, n_layers=8, n_heads=4)\n",
    "#model.load_weights('GPT_V1')\n",
    "model.generate_text('The', output_length=12, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.system('export XLA_FLAGS=–xla_gpu_cuda_data_dir=/opt/cuda')\n",
    "#model.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b02734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING: : 10it [4:33:29, 1640.92s/it]\n"
     ]
    }
   ],
   "source": [
    "history = model.train(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68b51e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'The mode of capital ins by a this and and  by and value as a it'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(dataset))\n",
    "#print(model(X)[0, 32, :])\n",
    "model.generate_text('The mode of capital ', output_length=16, top_k=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize(tokenizer.tokenize('I bims der Marx!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('a b c d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.detokenize([ 11, 462, 209, 754])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('GPT_V3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
